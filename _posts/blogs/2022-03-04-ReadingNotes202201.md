---
title: "Reading Notes 2022 Jan - Feb"
date: 2022-03-04
categories:
  - blog
tags:
  - learning
---

As in 2021, this series of blogs summarise the best data science-related posts Elise and I came across in the past two months during oue Friday and Sunday night reading time :)  

#### Experimentation  
1. **Experimentation at Netflix Series**: This series covers the basics of A/B tests ([Part 1](https://netflixtechblog.com/decision-making-at-netflix-33065fa06481) and [Part 2](https://netflixtechblog.com/what-is-an-a-b-test-b08cc1b57962)), core statistical concepts ([Part 3](https://netflixtechblog.com/interpreting-a-b-test-results-false-positives-and-statistical-significance-c1522d0db27a) and [Part 4](https://netflixtechblog.com/interpreting-a-b-test-results-false-negatives-and-power-6943995cf3a8)), how to build confidence in decisions based on A/B test results ([Part 5](https://netflixtechblog.com/building-confidence-in-a-decision-8705834e6fd8)), and experimentation use cases across different domains at Netflix ([Part 6](https://netflixtechblog.com/experimentation-is-a-major-focus-of-data-science-across-netflix-f67923f8e985)). This is by far the best experimentation reading series I have read, as it describes the underlying statistics and methodology clearly, meanwhile provides enough real-world examples and further readings  
2. [**Mindful Experimentation: Evaluate Recommendation System Performance using A/B Testing at Headspace**](https://medium.com/headspace-engineering/mindful-experimentation-evaluate-recommendation-system-performance-using-a-b-testing-at-headspace-3c8c05d0ae3b): This post walks through the architecture and various considerations to evaluate recommendations system performance at Headspace  
3. [**Experiment without the wait: Speeding up the iteration cycle with Offline Replay Experimentation**](https://medium.com/pinterest-engineering/experiment-without-the-wait-speeding-up-the-iteration-cycle-with-offline-replay-experimentation-7a4a95fa674b): How Pinterest uses the offline replay technique to speed up their experimentation  
4. [**Multiple Comparison: A Common Pitfall for A/B Testing**](https://towardsdatascience.com/multiple-comparison-a-common-pitfall-for-a-b-testing-d773f19a4a95): Why multiple comparison is a problem and its solutions  
5. [**Netflix: A Culture of Learning**](https://netflixtechblog.com/netflix-a-culture-of-learning-394bc7d0f94c): How Netflix established a culture of experimentation and causal inference  
6. [**Why It Matters Where You Randomize Users in A/B Experiments**](https://medium.com/@foundinblank/why-it-matters-where-you-randomize-users-in-a-b-experiments-5570c7585944): Simulates the impact on test duration with different user split point  
7. [**Don’t Use a T-Test for A/B Testing**](https://towardsdatascience.com/dont-use-a-t-test-for-a-b-testing-e4d2ef7ab9b6): How to use OLS to reduce the variance and reach statistical significance faster  
8. [**How to Double A/B Testing Speed with CUPED**](https://towardsdatascience.com/how-to-double-a-b-testing-speed-with-cuped-f80460825a90): Explains how we can use CUPED to speed up A/B testing  
9. [**Improve Your A/B Tests with 9 Lessons from the COVID-19 Vaccine Trials**](https://towardsdatascience.com/improve-your-a-b-tests-with-9-lessons-from-the-covid-19-vaccine-trials-8e270bf157d2): This posts talks about learnings from COVID-19 Vaccine trials and how we could apply them to A/B tests  
10. [**Estimating the Long-run Value We Give to Our Users through Experiment meta-analysis**](https://medium.com/@AnalyticsAtMeta/estimating-the-long-run-value-we-give-to-our-users-through-experiment-meta-analysis-6ddb9073b29b): How Meta uses meta-analysis method that combines a series of experimentation and linear regression to estimate long-run value  
11. [**Embrace Overlapping A/B Tests and Avoid the Dangers of Isolating Experiments**](https://blog.statsig.com/embracing-overlapping-a-b-tests-and-the-danger-of-isolating-experiments-cb0a69e09d3): A great comparison of sequential testing, overlapping testing, isolated testing, A/B/n testing, and multivariate testing  


#### Machine Learning  
1. [**Does Your Recommender System Evaluation Metric Meet Business Goals?**](https://medium.com/@decisionscientist/does-your-recommender-system-evaluation-metric-meet-business-goals-99939065d37b): Discusses various metrics to evaluate recommender system and how they align with the business goals  
2. [**6 Types of “Feature Importance” Any Data Scientist Should Master**](https://towardsdatascience.com/6-types-of-feature-importance-any-data-scientist-should-master-1bfd566f21c9): A great summary of different univariate and multivariate feature importance measures  
3. [**Model Interpretability in Risk Analytics**](https://medium.com/@nusfintech.ml/model-interpretability-in-risk-analytics-de5ac053b648): Interpret risk models with LIME, SHAP, iBreakDown, and Partial Dependence Plot (PDP)  
4. [**This is Machine Learning at Capital One**](https://medium.com/capital-one-tech/this-is-machine-learning-at-capital-one-9329838cbcd0): A catalog of machine learning-related blog posts at Capital One  
5. [**Managing Account Risk in Cash Product at Brex**](https://medium.com/brexeng/managing-account-risk-in-cash-product-at-brex-8d8315cd91e4): How we built the Account Risk model with active learning and how to make the model results more interpretable  


#### Analytics  
1. [**User Engagement Metrics in a nutshell**](https://medium.com/@simba.sp18/user-engagement-metrics-in-a-nutshell-72031449d340): This article summarizes common user engagement metrics and their definition  
2. [**Monetization metrics in a nutshell**](https://medium.com/@simba.sp18/monetization-metrics-in-a-nutshell-bfeba326c0ab): This article summarizes common monetization metrics and their definition  
3. [**Retention Analysis Framework**](https://towardsdatascience.com/retention-analysis-framework-4eb62933e2b): A beginner level reading on retention metrics and how to analyze retention  
4. [**Quantifying Product/Market Fit**](https://productcoalition.com/quantifying-product-market-fit-9c19c4d902b1): A basic framework to quantify your product/market fit  
5. [**Improving the Quality of Listings on Faire: A Case Study**](https://craft.faire.com/improving-the-quality-of-listings-on-faire-a-case-study-9afffc80801c): A great example of how Faire analyzed factors that impacting the quality of listings, and validated the insights  
6. [**Targeting Product Growth with Aha Moment Metrics**](https://productcoalition.com/targeting-product-growth-with-aha-moment-metrics-1d3889afc2b7): How to find the ‘Aha Moment’ metrics for startup product growth  
7. [**At a Startup, a Data Scientist must also be a Product Manager, and More**](https://towardsdatascience.com/at-a-startup-a-data-scientist-must-also-be-a-product-manager-and-more-19f96d3ec380): Role as the earliest data scientist at a startup  
8. [**Fixing Performance Regressions Before they Happen**](https://netflixtechblog.com/fixing-performance-regressions-before-they-happen-eab2602b86fe): How Netflix utilizes anomaly and changepoint detection to prevent performance regression  


